# 241880334 闵振昌 25FA3
## Problem 1
### Cumulative distribution function (CDF)
To be a distribution function, the function needs to satisfy:
- Monotone increasing
- Bounded

(1)$F(x)^{r}$
Monotone: easily we know F(x) is monotone, and so we assume that $F(x)^{k}$ is monotone increasing. For $F(x)^{k+1}$, for all $ x, y\in R$, $x\leq y$ we define a function $G(x, y)=F(x)^{k+1}-F(x)^{k}$, and by calculating its gradient we can know only when $F(x)^{k}=F(y)^{k}=0$, it reaches its minimum 0.
Therefore, $F(x)^{r}$ is monotone increasing.

Bounded: $\lim\limits_{x \to -\infty} F_X(x) = 0 \text{ and } \lim\limits_{x \to \infty} F_X(x) = 1$
So $\lim\limits_{x \to -\infty} F_X(x)^{r} = 0^{r}=0$, $\lim\limits_{x \to \infty} F_X(x)^{r} = 1^r=1$

Therefore, $F(x)^{r}$ is a distribution function.

(2)$1-(1-F(x))^r$
Monotone: easily we know $1-F(x)$ is monotone decreasing, and by (1) we can infer that $(1-F(x))^r$ is monotone decreasing, so $1-(1-F(x))^r$ is monotone increasing.

Bounded: $\lim\limits_{x \to -\infty} 1-(1-F(x))^r = 0$, $\lim\limits_{x \to \infty} 1-(1-F(x))^r = 1$

Therefore, $1-(1-F(x))^r$ is a distribution function.

(3)$F(x) + (1-F(x))\times log(1-F(x))$
Monotone: take the derivative, getting $F'(x) - F'(x) - F'(x)log(1-F(x))= -F'(x)log(1-F(x))$
We know F(x) is monotone increasing, so $F'(x) > 0$, and $1-F(x)\in [0, 1]$ so $-log(1-F(x)) > 0$, the original function is monotone increasing.

Bounded: $\lim\limits_{x \to -\infty} [F(x) + (1-F(x))\times log(1-F(x))] = \lim\limits_{F(x) \to 0} [F(x) + (1-F(x))\times log(1-F(x))]=0$
$\lim\limits_{x \to \infty} [F(x) + (1-F(x))\times log(1-F(x))] = \lim\limits_{F(x) \to 1} [F(x) + (1-F(x))\times log(1-F(x))]=1+\lim\limits_{F(x) \to 1} [(1-F(x))\times log(1-F(x))]=1+\lim\limits_{m \to 0} [m\times log(m)]=1$

Therefore, $F(x) + (1-F(x))\times log(1-F(x))$ is a distribution function.

(4)$(F(x)-1)\times e+exp(1-F(x))$
$(F(x)-1)\times e+exp(1-F(x))=e\times[exp(-F(x))+F(x)-1]$

Monotone: get its derivative, is $[-e^{-F(x)}+1]\times eF'(x)$ and easily we know this $\geq 0$
So $(F(x)-1)\times e+exp(1-F(x))$ is monotone increasing.

Bounded: $\lim\limits_{x \to -\infty} [(F(x)-1)\times e+exp(1-F(x))]= \lim\limits_{F(x) \to 0} e\times[exp(-F(x))+F(x)-1]=0$
$\lim\limits_{x \to \infty} [(F(x)-1)\times e+exp(1-F(x))]= \lim\limits_{F(x) \to 1} e\times[exp(-F(x))+F(x)-1]=e\times(e^{-1}+1-1)=1$

Therefore, $(F(x)-1)\times e+exp(1-F(x))$ is a distribution function.

### Probability mass function (PMF)
Easily we know that $T$ follows geometric distribution, so $P(T=k)=(1-p)^{k-1}p=\frac{1}{2^t}$.

And for a set $S_t$, the number of elements in it is $9\times 10^{t-1}$

As for a positive integer $n$ with $k$ digits $n\in S_k$, we need to calculate $P(N=n)$ for random variable $N$.
$P(N=n)=P(T=k)\times P(N=n|T=k)$, which means n is an element in $S_t$.
Therefore, $P(N=n)=\frac{1}{2^t}\times \frac{1}{9\times 10^{t-1}}=\frac{10}{9}(\frac{1}{20})^k$, which k is the number of digits of n.

### Transitive Coins
First we focus on coin A wins against coin B.
$P(A>B)=P(A=10)=\frac{3}{5}$
And so $P(B>C)=P(C=3)=\frac{3}{5}$
$P(A>C)=P(A=10)\times P(C=3)=\frac{3}{5}\times\frac{3}{5}=\frac{9}{25}$
Therefore, anyone who chooses first, the other can always choose a coin that minimizes the probability the first man of winning.
Assume he choose coin A, then the other can choose C because $P(A>C)$ is less than 0.5.
If he choose B, then the other can choose A because $P(B>A)=0.4$ is less than 0.5.
If he choose C, then the other can choose B because $P(C>B)=0.4$ is less than 0.5

So we should choose the second.

### Estimator
We select n adjacent elements $X_1, X_2, \dots, X_n$ from $\{X_r\}$, and define a new function $G(x) = \frac{1}{n}\sum_{i=1}^n I(X_r\leq x)$
So $F(x)$ represents the probability that a random variable is less than or equal to x, and $G(x)$ represents the proportion of elements in the selected n elements that are less than or equal to x.
By law of large numbers, we can know that when $n$ is large enough, $G(x)$ can approximate $F(x)$ well.

### Independence
We know that A, B and C follow geometric distribution, and we let $K_A, K_B, K_C$ be the number of rolls before getting a 6.
When getting a 6, the global count of rolls are $T_A, T_B, T_C$ respectively.
And $T_A=3K_A-2, T_B=3K_B-1, T_C=3K_C$

(1) We want $Pr(T_A<T_B<T_C)$
Easily we know $T_B-T_A=3(K_B-K_A)+1$ is positive only when $K_B-K_A\geq 0$.
Similarly, $T_C-T_B=3(K_C-K_B)+1$ is positive only when $K_C-K_B\geq 0$.

Thus $Pr(T_A<T_B<T_C)=Pr(K_A\leq K_B\leq K_C)=\sum\limits_{1 \leq a \leq b \leq c} \Pr(K_A = a) \Pr(K_B = b) \Pr(K_C = c) = p^3 \sum_{a \geq 1} \sum_{b \geq a} \sum_{c \geq b} q^{a-1} q^{b-1} q^{c-1}$, in which $p=\frac{1}{6}, q=\frac{5}{6}$

Using sum of geometric sequences we get $Pr(T_A<T_B<T_C)=\frac{p^3}{(1-q)(1-q^2)(1-q^3)}$
Substitute and evaluate, we get $Pr(T_A<T_B<T_C)=\frac{216}{1001}$

(2) We view rolling a 6 as a success, and rolling a non-6 as a failure.
Firstly we assume A to be the first to succeed. So rolling a success needs 3k fails and 1 success.
$$
\sum_{k=0}^\infty (\text{no successes in next } 3k \text{ trials}) \cdot (\text{success at next A trial}) = \sum_{k \geq 0} q^{3k} p = \frac{p}{1 - q^3}.
$$
And next it needs to succeed by B, and then C, as they're independent, the total probability is
$$
\Pr(\text{1st by A, 2nd by B, 3rd by C}) = \left( \dfrac{p}{1 - q^3} \right)^3.
$$
Evaluate and we get $\frac{46656}{753571}$

### Joint Distribution
We calculate the mixed second derivative of $F(x, y)$ with respect to $x$ and $y$.
$$
\frac{\partial^2 F}{\partial x \partial y}(x,y)=e^{-xy}(1-xy).
$$
When $xy > 1$, $\frac{\partial^2 F}{\partial x \partial y}(x,y) < 0$, so it is not a valid joint distribution function.

### Jensen's Entropy
