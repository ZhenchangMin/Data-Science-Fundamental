# 241880334 闵振昌 25FA5
## Problem 1
### Triangle neighbors
Fix a vertex $v$. The expected number of triangles containing $v$ is
$$
\binom{n-1}{2} p^3 = O(\frac{1}{n})
$$
For $v$ to be in two triangles, either they share one other vertex or are disjoint aside from $v$.
The expected number of such vertex-triangle pairs is $O(n\times ^2p^5+n\times n^4p^6)=O(n^{-2}+n^{-2})\rightarrow 0$.

By Markov’s inequality, no vertex lies in more than one triangle.

### Isolated vertices
Let $I_v$ be indicator that vertex $v$ is isolated.
$$
E(I_v)=(1-p)^{n-1}
$$
So
$$
\mu=E(\sum_v I_v)=n(1-p)^{n-1}
$$
For $p=\frac{c + \log n}{n}$,
$$
\mu_n=n[1-\frac{logn+c}{n}]^{n-1}\approx n e^{-(\log n + c)}=e^{-c}
$$
Thus $E[X]\to e^{-c}$.
For factorial moments, a set $S$ of $k$ vertices is isolated with probability
$$
(1-p)^{k(n-k)+\binom{k}{2}}\approx n^{-k}e^{-ck}
$$
Thus
$$
E[\binom{X}{k}]=\binom{n}{k}(1-p)^{k(n-k)+\binom{k}{2}}\approx \frac{n^k}{k!} n^{-k} e^{-ck}=\frac{e^{-ck}}{k!}
$$
By the method of moments, $X\xrightarrow{d}\text{ Poisson}(e^{-c})$

## Problem 2
### Jointly continuous

(1) If $U$ and $V$ are jointly continuous, they have a joint probability density function $f_{U,V}(u,v)$.
The event $U=V$ corresponds to the line $u=v$ in the plane, which has Lebesgue measure zero. Therefore:
$$
\Pr(U = V) = \iint_{u = v} f_{U,V}(u,v) \, du \, dv = 0
$$

(2) Let $ X \sim \text{Uniform}(0,1)$  and $Y = X $. Then  X  and  Y  are continuous random variables, $but \Pr(X = Y) = 1$. This does not contradict (1), because $(X, Y)$ is not jointly continuous — its probability mass is concentrated on the line $y = x$ , so there is no joint density. Part (1) only applies when a joint density exists.

### Distribution function
Let $\mathcal{F'}:\mathbb{R}\rightarrow[0, 1]$ satisfy:
- non-decreasing
- $\lim\limits_{x\rightarrow-\infty}F'(x)=0$, $\lim\limits_{x\rightarrow\infty}F'(x)=1$
- continuous everywhere
- not different at the same point

Can $F'$ be a cumulative distribution function (CDF) for some random variable?
Yes.A CDF only needs to be non-decreasing, right-continuous, and have the correct limits.

In fact if $F'$ is absolutely continuous, the random variable has a density; if not, it may correspond to a singular continuous distribution, but it is still a valid CDF.

### Density Function
Given that:
$$ f(x) = C \exp \left( -a x - e^{-x} \right), \quad x \in \mathbb{R}, $$

find $C$ such that $f$ is a probability density function.

Normalization requires:

$$ \int_{-\infty}^{\infty} f(x) \, dx = C \int_{-\infty}^{\infty} e^{-a x} e^{-e^{-x}} \, dx = 1. $$

Substitute $t = e^{-x}$, so $x = -\ln t$, $dx = -dt/t$.

When $x : -\infty \to \infty$, $t : \infty \to 0$. Swapping limits:

$$ \int_{-\infty}^{\infty} e^{-a x} e^{-e^{-x}} \, dx = \int_{0}^{\infty} t^{a-1} e^{-t} \, dt = \Gamma(a), $$

provided $a > 0$.

Thus:

$$ C \cdot \Gamma(a) = 1 \quad \Rightarrow \quad C = \frac{1}{\Gamma(a)}, \quad a > 0. $$

So for $a > 0$, $C = \frac{1}{\Gamma(a)}$ makes $f$ a PDF 

### iid
Let $\{X_r : r \geq 1\}$ be i.i.d. with distribution function $F$ such that $F(y) < 1$ for all $y$.
Define
$$
Y(y) = \min\{k : X_k > y\}.
$$
Then $Y(y)$ is geometric with success probability $p_y = 1 - F(y)$, and $\mathbb{E}[Y(y)] = 1/p_y$.
We want to show:
$$
\lim_{y \to \infty} \Pr\left(Y(y) \leq \beta \mathbb{E}[Y(y)]\right) = 1 - e^{-\beta}, \quad \beta > 0.
$$

Proof outline:
$$
\Pr(Y(y) > m) = [F(y)]^m.
$$
Let $m = \beta/p_y = \beta/(1 - F(y))$. Then:
$$
\Pr\left(Y(y) > \frac{\beta}{1 - F(y)}\right) = [F(y)]^{\beta/(1 - F(y))}.
$$
As $y \to \infty$, $F(y) \to 1$. Write $F(y) = 1 - \varepsilon$, then:
$$
(1 - \varepsilon)^{\beta/\varepsilon} \to e^{-\beta}.
$$
Hence:
$$
Pr(Y(y) \leq \frac{\beta}{1-F(y)})  = 1 - e^{-\beta}.
$$

### Tails and moments
For nonnegative $X$, we have $\mathbb{E}(X^r) = \int_0^\infty rx^{r-1} \Pr(X > x) \,dx$.

Since $\mathbb{E}(|X|^r)$ exists, applying this to $|X|$ gives
$$
\int_0^\infty rx^{r-1} \Pr(|X| > x) \,dx = \mathbb{E}(|X|^r) < \infty,
$$
so $\int_0^\infty x^{r-1} \Pr(|X| > x) \,dx < \infty$.

Also, since the integrand $x^{r-1} \Pr(|X| > x)$ is integrable, we must have $x^{r-1} \Pr(|X| > x) \to 0$ as $x \to \infty$, hence $x^r \Pr(|X| > x) \to 0$.

### Conditional expectation

By definition, $\psi(X) = \mathbb{E}(Y \mid X)$ is a function of $X$ such that for any measurable $g$,
$$
\mathbb{E}[\psi(X)g(X)] = \mathbb{E}[\mathbb{E}(Y \mid X)g(X)] = \mathbb{E}[g(X)\mathbb{E}(Y \mid X)].
$$

But by the law of total expectation, $\mathbb{E}[g(X)\mathbb{E}(Y \mid X)] = \mathbb{E}[\mathbb{E}(g(X)Y \mid X)] = \mathbb{E}[g(X)Y]$.

Thus $\mathbb{E}(\psi(X)g(X)) = \mathbb{E}(Yg(X))$.

### Correlated? Independent?
Let $ X \sim \text{Uniform}[-1,1]$.

$$
\mathbb{E}[Z_n] = \frac{1}{2} \int_{-1}^1 \cos(n\pi x) \, dx = \frac{\sin(n\pi)}{n\pi} = 0.
$$

For $ m \neq n $,

$$
\mathbb{E}[Z_m Z_n] = \frac{1}{2} \int_{-1}^1 \cos(m\pi x) \cos(n\pi x) \, dx.
$$

Using $ \cos A \cos B = \frac{1}{2}\left[\cos\left((m + n)\pi x\right) + \cos\left((m - n)\pi x\right)\right] $,
integral over symmetric interval of cosine with nonzero frequency is zero. So covariance is $ 0 \to $ uncorrelated.


But not independent: $ Z_1 $ and $ Z_2 $ are deterministic functions of $ X $, so knowing $ Z_1 $ may determine $ X $ up to sign and thus determine $ Z_2 $.


### Aliasing method
Any probability vector $p \in \mathbb{R}^n$ can be written as

$$
p = \frac{1}{n} \sum_{i=1}^n v_i
$$

where each $v_i$ is a probability vector with at most 2 nonzero entries.


Construction: Think of $p$ as a histogram. The method:

Pick $i$ with $p_i \leq 1/n$, $j$ with $p_j \geq 1/n$, form $v_1$ by putting mass $n p_i$ at $i$ and $1 - n p_i$ at $j$, update $p$ by removing $v_1 / n$, repeat.


Sampling method: Choose $I$ uniformly in $\{1, \dots, n\}$, then sample from the 2-point distribution $v_I$.

### Stochastic domination
$X$ stochastically dominates $Y$ means $F_X(t) \leq F_Y(t)$ for all $t$, equivalently $\Pr(X > t) \geq \Pr(Y > t)$ for all $t$.


If $\mathbb{E}[f(X)] \geq \mathbb{E}[f(Y)]$ for all non-decreasing $f$, take $f(x) = \mathbf{1}_{x>t}$ to get $\Pr(X > t) \geq \Pr(Y > t)$, so stochastic domination holds.


Conversely, if $X$ stochastically dominates $Y$, then by coupling there exists $X' \stackrel{d}{=} X, Y' \stackrel{d}{=} Y$ with $X' \geq Y'$ a.s. Then for non-decreasing $f$, $f(X') \geq f(Y')$ a.s., so $\mathbb{E}[f(X)] \geq \mathbb{E}[f(Y)]$.

## Problem 3
### Uniform Distribution (i)
Let $U \sim \text{Uniform}[0,1]$, $0 < q < 1$.

Define $X = 1 + \left\lfloor \frac{\ln U}{\ln q} \right\rfloor$.

Since $\ln U \leq 0$ and $\ln q < 0$, $\frac{\ln U}{\ln q} \geq 0$.

For $k \geq 1$,

$$
\mathbb{P}(X = k) = \mathbb{P}\left( 1 + \left\lfloor \frac{\ln U}{\ln q} \right\rfloor = k \right) = \mathbb{P}\left( k - 1 \leq \frac{\ln U}{\ln q} < k \right).
$$

Because $\ln q < 0$, inequalities reverse when multiplying:

$$
= \mathbb{P}(q^k < U \leq q^{k-1}) = q^{k-1} - q^k = q^{k-1}(1 - q).
$$

This is the geometric distribution with success probability $1 - q$ on $\{1, 2, \dots\}$.

### Uniform Distribution (ii)
Suppose $U = X + Y$ with $U \sim \text{Uniform}[0,1]$, $X,Y$ i.i.d.

Then $\mathbb{E}[U] = 1/2$, so $\mathbb{E}[X] = 1/4$.

Also $\text{Var}(U) = 1/12$, but $\text{Var}(X + Y) = 2\text{Var}(X)$, so $\text{Var}(X) = 1/24$.

But $X \in [0,1]$ a.s. (since $U \in [0,1]$ and $X,Y \geq 0$ by $U = X + Y$), so $\text{Var}(X) \leq \mathbb{E}[X^2] \leq \mathbb{E}[X] = 1/4$, no contradiction yet.


Better: Consider $\mathbb{P}(U \leq 1/2) = 1/2$.

But $\mathbb{P}(X + Y \leq 1/2) \geq \mathbb{P}(X \leq 1/4, Y \leq 1/4) = \left[\mathbb{P}(X \leq 1/4)\right]^2$.

If $X$ has mean $1/4$, by Markov $\mathbb{P}(X > 1/4) \leq 1$, but sharper: By uniformity of $U$, $F_U(t) = t$.

But $F_{X+Y}(t)$ is continuous and $F_{X+Y}(1) = 1$, $F_{X+Y}(0) = 0$, but for i.i.d. nonnegative $X,Y$, $F_{X+Y}(t)$ for small $t$ is $O(t^2)$ if $X$ has density at $0$, but here $F_{X+Y}(t) = t$ for all $t \in [0,1]$ implies $F_{X+Y}(t)$ linear, which is impossible for i.i.d. sum unless degenerate, contradiction.

### Uniform Distribution (iii)
Suppose uniform distribution on $(a, \infty)$ exists. Then PDF $f(x) = c$ for $x \geq a$.

But $\int_a^\infty c \, dx$ diverges unless $c = 0$, then total probability $0$. Contradiction.

So no uniform distribution on infinite intervals.

### Exponential distribution (i)

Let $X$ be continuous, memoryless: $\mathbb{P}(X > s + t \mid X > s) = \mathbb{P}(X > t)$ for $s,t \geq 0$.

Let $\bar{F}(t) = \mathbb{P}(X > t)$. Then $\bar{F}(s + t) = \bar{F}(s)\bar{F}(t)$.

The only continuous solution with $\bar{F}(0) = 1$, $\bar{F}(\infty) = 0$ is $\bar{F}(t) = e^{-\lambda t}$ for $\lambda > 0$.

Thus $X \sim \text{Exponential}(\lambda)$.

### Exponential distribution (ii)
Let $X \sim \text{Exp}(\lambda)$, $N = \lfloor X \rfloor$, $M = X - N \in [0,1)$.

For $n \in \mathbb{Z}_{\geq 0}$, $m \in [0,1)$,
$$
\mathbb{P}(N = n, M \leq m) = \mathbb{P}(n \leq X \leq n + m) = e^{-\lambda n} - e^{-\lambda(n+m)}.
$$

Also $\mathbb{P}(N = n) = \mathbb{P}(n \leq X < n + 1) = e^{-\lambda n} - e^{-\lambda(n+1)} = e^{-\lambda n}(1 - e^{-\lambda})$.

Thus
$$
\mathbb{P}(M \leq m \mid N = n) = \frac{e^{-\lambda n} - e^{-\lambda(n+m)}}{e^{-\lambda n}(1 - e^{-\lambda})} = \frac{1 - e^{-\lambda m}}{1 - e^{-\lambda}},
$$
independent of $n$. So $M$ and $N$ are independent.

$M$ has CDF $F_M(m) = \frac{1-e^{-\lambda m}}{1-e^{-\lambda}}$ for $m \in [0,1)$, density $f_M(m) = \frac{\lambda e^{-\lambda m}}{1-e^{-\lambda}}$.

$N$ is geometric: $\mathbb{P}(N = n) = (1 - e^{-\lambda})e^{-\lambda n}, \, n = 0, 1, 2, \ldots$.

### Waiting for offers

Offers $X_i \sim F$ i.i.d. Accept first offer $> K$.

Number of offers $N$ before sale is geometric with success probability $p = 1 - F(K)$.

So $\mathbb{E}[N] = \frac{1}{p} - 1 = \frac{F(K)}{1-F(K)}$ if counting only rejected offers before acceptance.

If including the accepted offer: $\mathbb{E}[N] = \frac{1}{1-F(K)}$.

Usually "number of offers received before I sell" means before acceptance:$\frac{F(k)}{1-F(k)}$

### Geometric distribution
Let $X \sim \text{exp}(\lambda)$, $N = \lfloor X \rfloor$.

For $k \geq 0$,
$$
\mathbb{P}(N = k) = \mathbb{P}(k \leq X < k + 1) = e^{-\lambda k} - e^{-\lambda(k+1)} = e^{-\lambda k}\left(1 - e^{-\lambda}\right).
$$

This is geometric distribution with success probability $p = 1 - e^{-\lambda}$ on $\{0, 1, 2, \ldots\}$.

### Poisson clocks
Let $N_1(t), \ldots, N_k(t)$ be independent Poisson processes rate $\lambda$.

Their superposition $N(t) = \sum_{i=1}^k N_i(t)$ is a Poisson process with rate $k\lambda$ by additive property of Poisson processes: independent increments, rate sums.

### Poissonian bears
Brown bears $B \sim \text{PP}(\beta)$, grizzly bears $G \sim \text{PP}(\gamma)$, independent.


(1) First bear arrival time: superposition is Poisson rate $\beta + \gamma$.

Probability first is brown: $\beta/(\beta + \gamma)$ by thinning property.


(2) Between two consecutive brown bears: time between browns $\sim \text{exp}(\beta)$.

Given inter-brown time $T$, number of grizzlies in that interval $\sim \text{Poisson}(\gamma T)$.

So
$$
\mathbb{P}(r \text{ grizzlies}) = \int_0^\infty e^{-\gamma T}\frac{(\gamma T)^r}{r!} \cdot \beta e^{-\beta T} dT = \frac{\beta \gamma^r}{r!} \int_0^\infty T^r e^{-(\beta+\gamma)T} dT.
$$

Integral $= r!/(\beta + \gamma)^{r+1}$, so
$$
\mathbb{P}(r \text{ grizzlies}) = \frac{\beta \gamma^r}{(\beta + \gamma)^{r+1}}.
$$

### Bivariate normal distributions (i)
From given density $f_{XY}(x,y)$, means: $\mathbb{E}[X] = \mu_1$, $\mathbb{E}[Y] = \mu_2$.

Variances: $\text{Var}(X) = \sigma_1^2$, $\text{Var}(Y) = \sigma_2^2$.

Covariance: $\text{Cov}(X,Y) = \rho \sigma_1 \sigma_2$ (since $Q(x,y)$ is standard bivariate normal form).


### Bivariate normal distributions (ii)
Let $X \sim N(0,1)$, $a > 0$,
$$
Y = 
\begin{cases} 
X & |X| < a \\
-X & |X| \geq a 
\end{cases}.
$$

Check $Y \sim N(0,1)$:

For $y \geq 0$,
$$
F_Y(y) = \mathbb{P}(Y \leq y) = \mathbb{P}(|X| < a, X \leq y) + \mathbb{P}(|X| \geq a, -X \leq y).
$$

For $y \geq a$, $\mathbb{P}(|X| < a, X \leq y) = \mathbb{P}(|X| < a)$ since $y \geq a$,
and $\mathbb{P}(|X| \geq a, -X \leq y) = \mathbb{P}(|X| \geq a)$ since $y \geq a > 0$, so sum = $1$? Wait, need careful symmetry:


Better: $Y$ is symmetric and $|Y| = |X|$, so $Y$ has same distribution as $X$ by symmetry of $N(0,1)$ and the transformation preserves measure.


Covariance:
$$
\rho(a) = \mathbb{E}[XY] = \mathbb{E}\left[X^2 \mathbf{1}_{|X|<a}\right] + \mathbb{E}\left[-X^2 \mathbf{1}_{|X|\geq a}\right].
$$
$$
= \mathbb{E}\left[X^2\right] - 2\mathbb{E}\left[X^2 \mathbf{1}_{|X|\geq a}\right] = 1 - 2 \int_{|x| \geq a} x^2 \varphi(x)dx.
$$

Let $\varphi(x) = \frac{1}{\sqrt{2\pi}}e^{-x^2/2}$,
$$
\rho(a) = 1 - 4 \int_{a}^{\infty} x^2 \varphi(x)dx.
$$

Integrate by parts: $\int_{a}^{\infty} x^2 \varphi(x)dx = \int_{a}^{\infty} x \cdot x\varphi(x)dx$,
$\int x\varphi(x)dx = -\varphi(x)$, so
$$
\int_{a}^{\infty} x^2 \varphi(x)dx = a\varphi(a) + \int_{a}^{\infty} \varphi(x)dx = a\varphi(a) + \bar{\Phi}(a),
$$
where $\bar{\Phi}(a) = 1 - \Phi(a)$.

Thus
$$
\rho(a) = 1 - 4\left[a\varphi(a) + \bar{\Phi}(a)\right].
$$


Pair $(X,Y)$ is not bivariate normal: e.g., $X + Y$ is not normal (it's $2X$ for $|X| < a$, $0$ for $|X| \geq a$, not normal).