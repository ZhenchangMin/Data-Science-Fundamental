# 241880334 闵振昌 25FA5
## Problem 1
### Triangle neighbors
Fix a vertex $v$. The expected number of triangles containing $v$ is
$$
\binom{n-1}{2} p^3 = O(\frac{1}{n})
$$
For $v$ to be in two triangles, either they share one other vertex or are disjoint aside from $v$.
The expected number of such vertex-triangle pairs is $O(n\times ^2p^5+n\times n^4p^6)=O(n^{-2}+n^{-2})\rightarrow 0$.

By Markov’s inequality, no vertex lies in more than one triangle.

### Isolated vertices
Let $I_v$ be indicator that vertex $v$ is isolated.
$$
E(I_v)=(1-p)^{n-1}
$$
So
$$
\mu=E(\sum_v I_v)=n(1-p)^{n-1}
$$
For $p=\frac{c + \log n}{n}$,
$$
\mu_n=n[1-\frac{logn+c}{n}]^{n-1}\approx n e^{-(\log n + c)}=e^{-c}
$$
Thus $E[X]\to e^{-c}$.
For factorial moments, a set $S$ of $k$ vertices is isolated with probability
$$
(1-p)^{k(n-k)+\binom{k}{2}}\approx n^{-k}e^{-ck}
$$
Thus
$$
E[\binom{X}{k}]=\binom{n}{k}(1-p)^{k(n-k)+\binom{k}{2}}\approx \frac{n^k}{k!} n^{-k} e^{-ck}=\frac{e^{-ck}}{k!}
$$
By the method of moments, $X\xrightarrow{d}\text{ Poisson}(e^{-c})$

## Problem 2
### Jointly continuous

(1) If $U$ and $V$ are jointly continuous, they have a joint probability density function $f_{U,V}(u,v)$.
The event $U=V$ corresponds to the line $u=v$ in the plane, which has Lebesgue measure zero. Therefore:
$$
\Pr(U = V) = \iint_{u = v} f_{U,V}(u,v) \, du \, dv = 0
$$

(2) Let $ X \sim \text{Uniform}(0,1)$  and $Y = X $. Then  X  and  Y  are continuous random variables, $but \Pr(X = Y) = 1$. This does not contradict (1), because $(X, Y)$ is not jointly continuous — its probability mass is concentrated on the line $y = x$ , so there is no joint density. Part (1) only applies when a joint density exists.

### Distribution function
Let $\mathcal{F'}:\mathbb{R}\rightarrow[0, 1]$ satisfy:
- non-decreasing
- $\lim\limits_{x\rightarrow-\infty}F'(x)=0$, $\lim\limits_{x\rightarrow\infty}F'(x)=1$
- continuous everywhere
- not different at the same point

Can $F'$ be a cumulative distribution function (CDF) for some random variable?
Yes.A CDF only needs to be non-decreasing, right-continuous, and have the correct limits.

In fact if $F'$ is absolutely continuous, the random variable has a density; if not, it may correspond to a singular continuous distribution, but it is still a valid CDF.

### Density Function
Given that:
$$ f(x) = C \exp \left( -a x - e^{-x} \right), \quad x \in \mathbb{R}, $$

find $C$ such that $f$ is a probability density function.

Normalization requires:

$$ \int_{-\infty}^{\infty} f(x) \, dx = C \int_{-\infty}^{\infty} e^{-a x} e^{-e^{-x}} \, dx = 1. $$

Substitute $t = e^{-x}$, so $x = -\ln t$, $dx = -dt/t$.

When $x : -\infty \to \infty$, $t : \infty \to 0$. Swapping limits:

$$ \int_{-\infty}^{\infty} e^{-a x} e^{-e^{-x}} \, dx = \int_{0}^{\infty} t^{a-1} e^{-t} \, dt = \Gamma(a), $$

provided $a > 0$.

Thus:

$$ C \cdot \Gamma(a) = 1 \quad \Rightarrow \quad C = \frac{1}{\Gamma(a)}, \quad a > 0. $$

So for $a > 0$, $C = \frac{1}{\Gamma(a)}$ makes $f$ a PDF 

### iid
Let $\{X_r : r \geq 1\}$ be i.i.d. with distribution function $F$ such that $F(y) < 1$ for all $y$.
Define
$$
Y(y) = \min\{k : X_k > y\}.
$$
Then $Y(y)$ is geometric with success probability $p_y = 1 - F(y)$, and $\mathbb{E}[Y(y)] = 1/p_y$.
We want to show:
$$
\lim_{y \to \infty} \Pr\left(Y(y) \leq \beta \mathbb{E}[Y(y)]\right) = 1 - e^{-\beta}, \quad \beta > 0.
$$

Proof outline:
$$
\Pr(Y(y) > m) = [F(y)]^m.
$$
Let $m = \beta/p_y = \beta/(1 - F(y))$. Then:
$$
\Pr\left(Y(y) > \frac{\beta}{1 - F(y)}\right) = [F(y)]^{\beta/(1 - F(y))}.
$$
As $y \to \infty$, $F(y) \to 1$. Write $F(y) = 1 - \varepsilon$, then:
$$
(1 - \varepsilon)^{\beta/\varepsilon} \to e^{-\beta}.
$$
Hence:
$$
Pr(Y(y) \leq \frac{\beta}{1-F(y)})  = 1 - e^{-\beta}.
$$

### Tails and moments
For nonnegative $X$, we have $\mathbb{E}(X^r) = \int_0^\infty rx^{r-1} \Pr(X > x) \,dx$.

Since $\mathbb{E}(|X|^r)$ exists, applying this to $|X|$ gives
$$
\int_0^\infty rx^{r-1} \Pr(|X| > x) \,dx = \mathbb{E}(|X|^r) < \infty,
$$
so $\int_0^\infty x^{r-1} \Pr(|X| > x) \,dx < \infty$.

Also, since the integrand $x^{r-1} \Pr(|X| > x)$ is integrable, we must have $x^{r-1} \Pr(|X| > x) \to 0$ as $x \to \infty$, hence $x^r \Pr(|X| > x) \to 0$.

### Conditional expectation

By definition, $\psi(X) = \mathbb{E}(Y \mid X)$ is a function of $X$ such that for any measurable $g$,
$$
\mathbb{E}[\psi(X)g(X)] = \mathbb{E}[\mathbb{E}(Y \mid X)g(X)] = \mathbb{E}[g(X)\mathbb{E}(Y \mid X)].
$$

But by the law of total expectation, $\mathbb{E}[g(X)\mathbb{E}(Y \mid X)] = \mathbb{E}[\mathbb{E}(g(X)Y \mid X)] = \mathbb{E}[g(X)Y]$.

Thus $\mathbb{E}(\psi(X)g(X)) = \mathbb{E}(Yg(X))$.

### Correlated? Independent?


### Aliasing method


### Stochastic domination