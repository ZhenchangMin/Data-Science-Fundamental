# 241880334 闵振昌 25FA5
## Problem 1
### Proportional betting

- Setup: Let $F_0>0$ and independent multipliers $Y_k$ with $\mathbb P(Y_k=1.30)=\mathbb P(Y_k=0.75)=1/2$, so $F_n = F_0\,\prod_{k=1}^n Y_k$.

- Expectation: $\mathbb E[Y_k] = (1.30+0.75)/2 = 1.025$. Independence gives $\mathbb E[F_n] = F_0\,(\mathbb E Y)^n = F_0\cdot 1.025^n \to \infty$ as $n\to\infty$.

- Almost-sure decay: Put $S_n = \log F_n = \log F_0 + \sum_{k=1}^n \log Y_k$. By the Strong Law of Large Numbers,
	$$
	\frac{1}{n}\sum_{k=1}^n \log Y_k \;\xrightarrow{a.s.}\; \mathbb E[\log Y]
	= \tfrac12\log(1.30)+\tfrac12\log(0.75) < 0.
	$$
	Hence $S_n\to -\infty$ a.s., and therefore $F_n = e^{S_n}\to 0$ almost surely.

### Entropy

Let the partition of $[0,1]$ have lengths $p_1,\dots,p_n$ and entropy $h = -\sum_{i=1}^n p_i\log p_i$. Draw $X_1,X_2,\dots$ i.i.d. $\mathrm{Unif}[0,1]$, and let $Z_m^{(i)}$ be the count among $X_1,\dots,X_m$ falling in interval $i$.

- Multinomial structure: $(Z_m^{(1)},\dots,Z_m^{(n)}) \sim \mathrm{Multinomial}(m; p_1,\dots,p_n)$.
- Empirical frequencies: $\hat p_i := Z_m^{(i)}/m \xrightarrow{a.s.} p_i$ by the Strong Law of Large Numbers.
- Define $R_m = \prod_{i=1}^n p_i^{Z_m^{(i)}}$. Then
	$$
	\frac{1}{m}\log R_m = \sum_{i=1}^n \frac{Z_m^{(i)}}{m}\,\log p_i
	\;\xrightarrow[m\to\infty]{a.s.}\; \sum_{i=1}^n p_i\log p_i = -h.
	$$
Thus $m^{-1}\log R_m \to -h$ almost surely.

### Mobilizing a Supermajority

Setup: For each of $n$ baseline individuals, (i) attends with probability $\tau$; (ii) if attending, votes Yes with probability $p$ (No with $1-p$). A proposal is accepted if the fraction of Yes among attendees is at least $\theta\in(1/2,1)$. A campaign can add $m$ sure attendees who vote Yes.

Notation:
- Baseline attendees $A \sim \mathrm{Bin}(n,\tau)$.
- Baseline Yes votes among attendees, given $A=k$, are $Y\mid A=k \sim \mathrm{Bin}(k,p)$.
- After adding $m$ supporters, acceptance is $(Y+m)/(A+m) \ge \theta$.

Exact acceptance probability for a given $m$:
$$
P_{\text{pass}}(m)
= \sum_{k=0}^n \binom{n}{k} \tau^k(1-\tau)^{n-k}\;\Pr\!\Big[\mathrm{Bin}(k,p) \ge \big\lceil \theta(k+m) - m \big\rceil\Big].
$$
The minimal $m$ that achieves $P_{\text{pass}}(m) \ge 1-\delta$ can be found exactly by increasing $m$ until the sum exceeds $1-\delta$ (numerically tractable for moderate $n$).

Closed-form normal approximation (CLT): Define per-person contribution $T_i := \mathbf{1}\{\text{attend+Yes}\} - \theta\,\mathbf{1}\{\text{attend}\}$, so the baseline margin is $T := \sum_{i=1}^n T_i = Y - \theta A$. Passing with $m$ supporters is
$$
T + (1-\theta)m \ge 0.
$$
Here
$$
\mu = \mathbb E[T_i] = \tau(p-\theta),\qquad
\sigma^2 = \mathrm{Var}(T_i) = \tau\big[p(1-\theta)^2 + (1-p)\theta^2\big] - \tau^2(p-\theta)^2.
$$
Approximating $T \approx \mathcal N(n\mu,\, n\sigma^2)$, the $\delta$-lower quantile is $n\mu + z_{\delta}\,\sigma\sqrt n$ (with $z_{\delta}=\Phi^{-1}(\delta)$). Ensuring $\Pr[T \ge -(1-\theta)m] \ge 1-\delta$ yields
$$
m_{\\text{CLT}} \approx \Bigg\lceil \max\Big\{0,\; \frac{-n\mu - z_{\delta}\,\sigma\sqrt n}{1-\theta} \Big\} \Bigg\rceil.
$$
Interpretation: If $p\ge\theta$, then $\mu\ge0$ and often $m\approx0$ suffices; if $p<\theta$, the formula gives the extra Yes votes needed to overcome the expected shortfall plus a $\delta$-level safety margin.

## Problem 2
### Tossing coins
Let $X$ be the number of throws needed to obtain $n$ heads when tossing a fair coin. Then $X$ is a sum of $n$ i.i.d. geometric$(p=1/2)$ variables, i.e., a negative-binomial $\mathrm{NB}(n,1/2)$ random variable with moment generating function
$$
M_X(t) = \Big( \frac{\tfrac12 e^t}{1-\tfrac12 e^t} \Big)^n,\quad 0<t<\ln 2.
$$
By Chernoff's method, for any $t\in(0,\ln 2)$,
$$
\Pr\big[X\ge x\big] \le e^{-t x}\,M_X(t).
$$
Set $x=2n+\Delta$ with $\Delta>0$. Minimizing the RHS over $t$ is equivalent to minimizing over $y=e^t\in(1,2)$ the function
$$
f(y)=-x\ln y + n\big[\ln(\tfrac{y}{2}) - \ln(1-\tfrac{y}{2})\big].
$$
Writing $u=y/2$ and optimizing gives $u^*=(n+\Delta)/(2n+\Delta)$ and
$$
\ln\Pr[X\ge 2n+\Delta] \le - (3n+\Delta)\ln 2 + (2n+\Delta)\ln(2n+\Delta) - (n+\Delta)\ln(n+\Delta) - n\ln n.
$$
Using the inequality $\ln(1+x)\le x - x^2/2 + x^3/3$ for $x\in[0,1]$ and the constraint $0<\Delta\le 2n$ (equivalently $0<\delta<\sqrt{4n/\ln n}$ when $\Delta=\delta\sqrt{n\ln n}$), one checks that
$$
\ln\Pr[X\ge 2n+\Delta] \le -\frac{\Delta^2}{6n}.
$$
Therefore for any $0<\delta<\sqrt{\frac{4n}{\ln n}}$,
$$
\Pr\big[ X > 2n + \delta\sqrt{n\ln n} \big] \le \exp\!\Big( -\frac{\delta^2}{6}\,\ln n \Big) = n^{-\delta^2/6}.
$$

### k-th moment bound

Assume $\mathbb E[e^{t|X|}]<\infty$ for some $t>0$ and $\mathbb E[X]=0$.

- Chernoff bound: $\Pr(|X|\ge \delta) \le \inf_{s>0} e^{-s\delta}\,\mathbb E[e^{s|X|}]$.
- $k$-th moment bound: $\Pr(|X|\ge \delta) \le \mathbb E[|X|^k]/\delta^k$.

1) Existence of a $k$ matching Chernoff (probabilistic method). Let $K\sim\mathrm{Poisson}(s\delta)$ be independent of $X$. Then
$$
\mathbb E\Big[\frac{|X|^K}{\delta^K}\Big] = \mathbb E\big[ e^{s(|X|-\delta)} \big] = e^{-s\delta}\,\mathbb E\big[e^{s|X|}\big].
$$
Since $\Pr(|X|\ge\delta)\le \mathbb E[|X|^k]/\delta^k$ for each fixed $k$, averaging this bound over $K$ yields exactly the Chernoff bound for the same $s$. Hence there exists a realization $k$ of $K$ such that
$$
\Pr(|X|\ge\delta) \le \frac{\mathbb E[|X|^k]}{\delta^k} \le e^{-s\delta}\,\mathbb E[e^{s|X|}]\quad \text{for that }k.
$$
Minimizing over $s>0$ shows that for every $\delta$, one can choose an integer $k$ so that the $k$-th moment bound is no weaker than the optimal Chernoff bound.

2) Why still prefer Chernoff? The Chernoff method
- provides smoothly tunable bounds via a continuous parameter $s$ rather than discrete $k$;
- avoids factorial/large-$k$ constants and is usually sharper for moderate deviations;
- requires only finiteness of the mgf near $0$, whereas high-order moments may not exist or be hard to evaluate;
- yields clean, distribution-agnostic exponential tails with standard templates (Hoeffding/Bernstein/KL forms).

### Densest induced subgraph in random graph
Let $G\sim G(n,1/2)$. For $S\subseteq [n]$, denote by $e(S)$ the number of edges with both endpoints in $S$ and define
$$
\mathrm{dens}(S):=\frac{2e(S)}{|S|}\quad(\text{average degree in the induced subgraph}).
$$
Fix $s=|S|\ge 2$. Then $e(S)\sim \mathrm{Bin}\!\big(\binom{s}{2},\tfrac12\big)$ with mean $\mu_s=\tfrac12\binom{s}{2}$ and variance $\sigma_s^2=\tfrac14\binom{s}{2}$. By Bernstein (or Hoeffding), for any $t>0$,
$$
\Pr\Big[\mathrm{dens}(S)\ge \frac{s}{2}+t\Big]
\;=\;\Pr\Big[e(S)\ge \mu_s + \frac{t s}{2}\Big]
\;\le\; \exp\Big( -\frac{t^2 s^2}{2\binom{s}{2}} \Big)
\;\le\; \exp\Big( -\frac{t^2}{s} \Big).
$$
Apply a union bound over all $\binom{n}{s}$ subsets of size $s$:
$$
\Pr\Big[\exists S:\,\mathrm{dens}(S)\ge \frac{s}{2}+t\Big]
\le \binom{n}{s}\,\exp\Big(-\frac{t^2}{s}\Big)
\le \exp\Big( s\ln\frac{en}{s} - \frac{t^2}{s} \Big).
$$
Choose $t=C\sqrt n$ with an absolute constant $C>0$ large enough; then the exponent is $\le -\ln 3$ for all $s\in[2,n]$, so summing over $s$ gives
$$
\Pr\big[\max_{S}\,\mathrm{dens}(S)\le n/2 + C\sqrt n\big] \ge 2/3,
$$
that is, with probability at least $2/3$, the densest induced subgraph in $G(n,1/2)$ has average degree at most $n/2 + C n^{1/2}$.

## Problem 3
### High-dimensional random walk (martingale)

Let $(X_t)_{t\ge0}$ be a random walk in $\mathbb R^n$ with increments $\Delta_t:=X_{t+1}-X_t$ such that $\mathbb E[\Delta_t\mid \mathcal F_t]=0$ for all $t$ (unbiased step in every dimension, possibly with dependent coordinates and arbitrary distributions), where $\mathcal F_t$ is the natural filtration. Then
$$
\mathbb E[X_{t+1}\mid \mathcal F_t] = X_t + \mathbb E[\Delta_t\mid \mathcal F_t] = X_t,
$$
so $(X_t)$ is a martingale in any dimension. Coordinate-wise, each $X_t^{(i)}$ is a martingale by the same argument.

### Pólya's urn martingale

Initially $r>0$ red and $b>0$ blue balls. At time $n$, draw one ball uniformly, return it and add another of the same color. Let $R_n$ be the number of red balls after $n$ draws and define
$$
Y_n := \frac{R_n}{r+b+n}.
$$
Conditional on $\mathcal F_{n-1}$, $R_n = R_{n-1} + I_n$ where $I_n\in\{0,1\}$ is the indicator of drawing red, with
$$
\mathbb P(I_n=1\mid \mathcal F_{n-1}) = \frac{R_{n-1}}{r+b+n-1}.
$$
Hence
$$
\mathbb E[Y_n\mid \mathcal F_{n-1}] = \frac{\mathbb E[R_{n-1}+I_n\mid \mathcal F_{n-1}]}{r+b+n}
= \frac{R_{n-1} + R_{n-1}/(r+b+n-1)}{r+b+n}
= \frac{R_{n-1}}{r+b+n-1} = Y_{n-1}.
$$
Thus $(Y_n)$ is a martingale.

### Optional stopping for 1-D symmetric walk

Let $S_n=a+\sum_{r=1}^n X_r$ where $X_r\in\{\pm1\}$ are i.i.d. symmetric. Compute
$$
\mathbb E[S_{n+1}^3\mid \mathcal F_n] = S_n^3 + 3S_n,
$$
so $S_{n+1}^3 - S_n^3 - 3S_n$ has zero conditional mean and
$$
M_n := \sum_{r=0}^n S_r - \tfrac13 S_n^3
$$
is a martingale. Let $T$ be the hitting time of $\{0,K\}$ with $0<a<K$. Under standard integrability conditions, optional stopping yields
$$
\mathbb E\Big[\sum_{r=0}^T S_r - \tfrac13 S_T^3\Big] = \mathbb E[M_T] = \mathbb E[M_0] = a - \tfrac13 a^3.
$$
For the simple symmetric walk, $\mathbb P(S_T=K)=a/K$, therefore $\mathbb E[S_T^3]=K^3\cdot (a/K)=aK^2$. Rearranging gives
$$
\mathbb E\Big[\sum_{r=0}^T S_r\Big] = \tfrac13 aK^2 + a - \tfrac13 a^3 = \tfrac13 (K^2-a^2)\,a + a.
$$

### Random walk on a graph: stationary distribution

Let $G$ be a finite connected simple undirected graph with $\eta$ edges and degrees $d_v$. The simple random walk moves from $v$ to a uniformly chosen neighbor; hence $P_{v,u}=1/d_v$ if $u\sim v$. Define $\pi(v)=d_v/(2\eta)$. Then for any edge $u\sim v$,
$$
\pi(v)P_{v,u}=\frac{d_v}{2\eta}\cdot\frac{1}{d_v}=\frac{1}{2\eta}=\frac{d_u}{2\eta}\cdot\frac{1}{d_u}=\pi(u)P_{u,v}.
$$
Thus detailed balance holds and $\pi$ is stationary.

### Reversibility versus periodicity

Yes. A reversible chain can be periodic. Example: the simple random walk on any bipartite graph (e.g., the path $\{0,1,\dots,n\}$ or an even cycle) is reversible with respect to degree measure, yet has period $2$ because it alternates between the two partitions.

### Metropolis–Hastings algorithm

1) With neighborhood structure $N(x)$ and $M\ge \max_x |N(x)|$, define
$$
P_{x,y}=\begin{cases}
	frac{1}{M}\min\{1,\pi_y/\pi_x\}, & y\in N(x),\, y\ne x,\\[4pt]
0, & y\notin N(x),\, y\ne x,\\[4pt]
1-\sum_{z\ne x} P_{x,z}, & y=x.
\end{cases}
$$
Assuming $y\in N(x)\iff x\in N(y)$ (undirected proposals),
$$
\pi_x P_{x,y} = \frac{\pi_x}{M}\min\{1,\pi_y/\pi_x\} = \frac{1}{M}\min\{\pi_x,\pi_y\} = \pi_y P_{y,x},
$$
so detailed balance holds and $\pi$ is stationary. Irreducibility and aperiodicity are assumed.

2) Target on positive integers: $\pi_i = 1/(S i^2)$ where $S=\sum_{i\ge1} i^{-2}=\pi^2/6$. Use linear neighbors: for $i>1$, $N(i)=\{i-1,i+1\}$ and $N(1)=\{2\}$. Take $M=2$. Then for $i\ge2$,
$$
P_{i,i-1}=\tfrac12\min\Big\{1,\frac{\pi_{i-1}}{\pi_i}\Big\}=\tfrac12\min\Big\{1,\Big(\frac{i}{i-1}\Big)^2\Big\}=\tfrac12,
\quad P_{i,i+1}=\tfrac12\min\Big\{1,\frac{\pi_{i+1}}{\pi_i}\Big\}=\tfrac12\min\Big\{1,\Big(\frac{i}{i+1}\Big)^2\Big\}=\tfrac12\Big(\frac{i}{i+1}\Big)^2.
$$
Set $P_{i,i}=1-P_{i,i-1}-P_{i,i+1}$. For $i=1$,
$$
P_{1,2}=\tfrac12\min\Big\{1,\frac{\pi_2}{\pi_1}\Big\}=\tfrac12\Big(\frac{1}{2}\Big)^2=\tfrac18,\quad P_{1,1}=1-P_{1,2}=\tfrac{7}{8}.
$$
This MH chain is irreducible and reversible w.r.t. $\pi_i\propto i^{-2}$, giving the desired stationary distribution.
